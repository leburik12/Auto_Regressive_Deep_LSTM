{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leburik12/Auto_Regressive_Deep_LSTM/blob/main/Auto_Regressive_Deep_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR2EIsd6UKKx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import requests\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkc31tCIUSxW"
      },
      "outputs": [],
      "source": [
        "def fetch_gutenberg_text(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def preprocess_text( text: str,\n",
        "                    seq_length: int = 100,\n",
        "                    batch_size: int = 32) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
        "    \"\"\"Convert text to sequences for RNN training.\"\"\"\n",
        "    # Create character vocabulary\n",
        "    chars = sorted(list(set(text)))\n",
        "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Convert text to indices\n",
        "    indices = [char_to_idx[ch] for ch in text]\n",
        "\n",
        "    # Create sequences\n",
        "    num_seqs = len(indices) // seq_length\n",
        "    X = np.zeros((num_seqs, seq_length), dtype=np.int64)\n",
        "    y = np.zeros((num_seqs, seq_length), dtype=np.int64)\n",
        "\n",
        "    for i in range(num_seqs):\n",
        "        start_idx = i * seq_length\n",
        "        end_idx = start_idx + seq_length\n",
        "        X[i] = indices[start_idx:end_idx]\n",
        "        y[i] = indices[start_idx+1:end_idx+1]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_tensor = torch.from_numpy(X)\n",
        "    y_tensor = torch.from_numpy(y)\n",
        "\n",
        "    # Create batches\n",
        "    num_batches = num_seqs // batch_size\n",
        "    X_batches = X_tensor[:num_batches*batch_size].view(batch_size, -1, seq_length)\n",
        "    y_batches = y_tensor[:num_batches*batch_size].view(batch_size, -1, seq_length)\n",
        "\n",
        "    return X_batches, y_batches, char_to_idx, idx_to_char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skBHxaoVUjVo"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    \"\"\"Basic LSTM Cell from scratch.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Concatenated weights for the four gates: [i, f, g, o]\n",
        "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size) * 0.01)\n",
        "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size) * 0.01)\n",
        "        self.bias_ih = nn.Parameter(torch.zeros(4 * hidden_size))\n",
        "        self.bias_hh = nn.Parameter(torch.zeros(4 * hidden_size))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass for a single timestep.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, input_size)\n",
        "            state: Tuple (h_prev, c_prev) where each is (batch_size, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            h_new: New hidden state (batch_size, hidden_size)\n",
        "            (h_new, c_new): New state tuple for next timestep\n",
        "        \"\"\"\n",
        "        h_prev, c_prev = state\n",
        "\n",
        "        # Compute all gates in one matrix multiplication (efficient)\n",
        "        gates = (torch.matmul(x, self.weight_ih.t()) + self.bias_ih) + \\\n",
        "                (torch.matmul(h_prev, self.weight_hh.t()) + self.bias_hh)\n",
        "\n",
        "        # Split into four gates\n",
        "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(input_gate)    # How much new info to add\n",
        "        f = torch.sigmoid(forget_gate)   # How much old cell to keep\n",
        "        g = torch.tanh(cell_gate)        # Candidate new cell values\n",
        "        o = torch.sigmoid(output_gate)   # How much cell to expose as hidden\n",
        "\n",
        "        c_new = f * c_prev + i * g        # Core: additive update to cell\n",
        "        h_new = o * torch.tanh(c_new)     # Hidden is gated view of cell\n",
        "\n",
        "        return h_new, c_new\n",
        "\n",
        "    def init_state(self, batch_size: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Initialize hidden and cell states with zeros.\"\"\"\n",
        "        device = self.weight_ih.device\n",
        "        return (torch.zeros(batch_size, self.hidden_size, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_size, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JTLPcqdUlrw"
      },
      "outputs": [],
      "source": [
        "class DeepRNNLayer(nn.Module):\n",
        "    \"\"\"A single layer of a Deep RNN that processes sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_size: int,\n",
        "                 cell_type: str = 'rnn', activation: str = 'tanh'):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "        if cell_type == 'rnn':\n",
        "            self.cell = RNNCell(input_size, hidden_size, activation)\n",
        "        elif cell_type == 'lstm':\n",
        "            self.cell = LSTMCell(input_size, hidden_size)\n",
        "        elif cell_type == 'gru':\n",
        "            self.cell = nn.GRUCell(input_size, hidden_size)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported cell type: {cell_type}\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor,\n",
        "                h_prev: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through the RNN layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
        "            h_prev: Initial hidden state\n",
        "\n",
        "        Returns:\n",
        "            outputs: Sequence of hidden states (batch_size, seq_len, hidden_size)\n",
        "            h_last: Last hidden state\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Initialize hidden state if not provided\n",
        "        if h_prev is None:\n",
        "            if self.cell_type == 'lstm':\n",
        "                h_prev = self.cell.init_state(batch_size)\n",
        "            else:\n",
        "                h_prev = self.cell.init_hidden(batch_size)\n",
        "\n",
        "        # Process sequence\n",
        "        outputs = []\n",
        "        h_current = h_prev\n",
        "\n",
        "        for t in range(seq_len):\n",
        "          x_t = x[:, t, :]\n",
        "          if self.cell_type == 'lstm':\n",
        "            h, c = h_current\n",
        "            h_new, c_new = self.cell(x_t, (h,c))\n",
        "            h_current = (h_new, c_new)\n",
        "            outputs.append(h_new)\n",
        "          else:\n",
        "            h_current = self.cell(x_t, h_current)\n",
        "            outputs.append(h_current)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "\n",
        "        if self.cell_type == 'lstm':\n",
        "          h_last = h_current\n",
        "        else:\n",
        "          h_last = h_current[0]\n",
        "\n",
        "        return outputs, h_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfe5iWS5Usp7"
      },
      "outputs": [],
      "source": [
        "class DeepRNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size: int, hidden_size: int, num_layers: int,\n",
        "               output_size: int, cell_type: str = 'rnn',\n",
        "               activation: str = 'tanh', dropout: float = 0.0):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.output_size = output_size\n",
        "    self.cell_type = cell_type\n",
        "\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      layer_input_size = input_size if i == 0 else hidden_size\n",
        "      self.layers.append(\n",
        "          DeepRNNLayer(layer_input_size, hidden_size, cell_type, activation)\n",
        "      )\n",
        "\n",
        "    # Dropout for regularization (applied between RNN layers)\n",
        "    self.dropout = nn.Dropout(dropout) if dropout > 0.0 else None\n",
        "\n",
        "    # Output layer\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x: torch.Tensor,\n",
        "              hs: Optional[List[torch.Tensor]] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
        "\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "\n",
        "    if hs is None:\n",
        "      hs = [None] * self.num_layers\n",
        "\n",
        "    hs_list = []\n",
        "    current_input = x\n",
        "\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      layer_output, h_last = layer(current_input, hs[i])\n",
        "\n",
        "      if self.dropout is not None and i < self.num_layers - 1:\n",
        "        layer_output = self.dropout(layer_output)\n",
        "\n",
        "      current_input = layer_output\n",
        "      hs_list.append(h_last)\n",
        "\n",
        "    batch_size, seq_len, hidden_size = layer_output.shape\n",
        "    outputs_flat = layer_output.reshape(-1, hidden_size)\n",
        "    outputs_flat = self.fc(outputs_flat)\n",
        "\n",
        "    outputs = outputs_flat.reshape(batch_size, seq_len, self.output_size)\n",
        "\n",
        "    return outputs, hs_list\n",
        "\n",
        "\n",
        "  def init_hidden(self, batch_size: int) -> List[torch.Tensor]:\n",
        "    hs = []\n",
        "    for layer in self.layers:\n",
        "      if layer.cell_type == 'lstm':\n",
        "        hs.append(layer.cell.init_state(batch_size))\n",
        "      else:\n",
        "        hs.append(layer.cell.init_hidden(batch_size))\n",
        "    return hs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_u1k8L5Uu1T"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for text generation tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, text: str, seq_length: int = 100):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "        self.indices = [self.char_to_idx[ch] for ch in text]\n",
        "        self.num_samples = len(self.indices) - seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq = self.indices[idx:idx + self.seq_length]\n",
        "        target_seq = self.indices[idx + 1:idx + self.seq_length + 1]\n",
        "        return torch.tensor(input_seq), torch.tensor(target_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irl-lU-qUwZA"
      },
      "outputs": [],
      "source": [
        "class DeepRNNTrainer:\n",
        "    \"\"\"Trainer for Deep RNN models.\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "                 criterion: nn.Module, device: str = 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "\n",
        "    def train_epoch(self, data_loader: torch.utils.data.DataLoader) -> float:\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            inputs = inputs.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "\n",
        "            # Add embedding dimension\n",
        "            batch_size, seq_len = inputs.shape\n",
        "            inputs_one_hot = F.one_hot(inputs, num_classes=self.model.input_size).float()\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs, _ = self.model(inputs_one_hot)\n",
        "\n",
        "            # Reshape for loss computation\n",
        "            outputs = outputs.reshape(-1, self.model.output_size)\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * targets.shape[0]\n",
        "            total_samples += targets.shape[0]\n",
        "\n",
        "        return total_loss / total_samples\n",
        "\n",
        "    def generate_text(self, start_text: str, length: int = 100,\n",
        "                     temperature: float = 1.0) -> str:\n",
        "        \"\"\"Generate text using the trained model.\"\"\"\n",
        "        self.model.eval()\n",
        "        generated = start_text\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Convert start text to tensor\n",
        "            input_seq = torch.tensor([\n",
        "                self.model.fetcher.char_to_idx[ch]\n",
        "                for ch in start_text[-self.model.seq_length:]]\n",
        "            ).unsqueeze(0).to(self.device)\n",
        "\n",
        "            hidden = self.model.init_hidden(1)\n",
        "\n",
        "            for _ in range(length):\n",
        "                # Prepare input\n",
        "                input_one_hot = F.one_hot(input_seq[:, -1:],\n",
        "                                        num_classes=self.model.input_size).float()\n",
        "\n",
        "                # Forward pass\n",
        "                output, hidden = self.model(input_one_hot, hidden)\n",
        "\n",
        "                # Get probabilities\n",
        "                output = output[:, -1, :] / temperature\n",
        "                probabilities = F.softmax(output, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "                # Sample next character\n",
        "                next_idx = np.random.choice(len(probabilities), p=probabilities)\n",
        "                next_char = self.model.fetcher.idx_to_char[next_idx]\n",
        "\n",
        "                generated += next_char\n",
        "\n",
        "                # Update input sequence\n",
        "                next_idx_tensor = torch.tensor([[next_idx]]).to(self.device)\n",
        "                input_seq = torch.cat([input_seq[:, 1:], next_idx_tensor], dim=1)\n",
        "\n",
        "        return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plz2GXQLUy8T",
        "outputId": "bc8a3ff0-38cd-463e-9ca3-7e4b4bd37078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text length: 5359444\n",
            "Unique characters: 100\n"
          ]
        }
      ],
      "source": [
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "text = fetch_gutenberg_text(url)\n",
        "\n",
        "assert text is not None\n",
        "print(\"Text length:\", len(text))\n",
        "print(f\"Unique characters: {len(set(text))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm-g1f4iU0pa"
      },
      "outputs": [],
      "source": [
        "MAX_CHARS = 10_000\n",
        "text = text[:MAX_CHARS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UZn98TIU28J"
      },
      "outputs": [],
      "source": [
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dataset = TextDataset(text, seq_length=SEQ_LENGTH)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0igFlfA-U4Wz"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "vocab_size = len(dataset.chars)\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "cell_type = 'lstm'  # Using LSTM for better memory\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujUAdReWU68l",
        "outputId": "254a0885-b194-4776-95c1-092382d6df5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß† Model Configuration:\n",
            "  Vocabulary size: 73\n",
            "  Hidden size: 128\n",
            "  Number of layers: 2\n",
            "  Cell type: lstm\n",
            "  Sequence length: 100\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nüß† Model Configuration:\")\n",
        "print(f\"  Vocabulary size: {vocab_size}\")\n",
        "print(f\"  Hidden size: {hidden_size}\")\n",
        "print(f\"  Number of layers: {num_layers}\")\n",
        "print(f\"  Cell type: {cell_type}\")\n",
        "print(f\"  Sequence length: {SEQ_LENGTH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Zm8K8PU8bX"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = DeepRNN(\n",
        "    input_size=vocab_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    output_size=vocab_size,\n",
        "    cell_type=cell_type,\n",
        "    dropout=dropout\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1SyJVh_U9yu"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize trainer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = DeepRNNTrainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion,\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KYZTw_HVAAS"
      },
      "outputs": [],
      "source": [
        "model.fetcher = dataset\n",
        "model.seq_length = SEQ_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F9_6G7fVCdM",
        "outputId": "0fed9398-8d4e-4330-8da3-73c5e745cf04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèãÔ∏è  Training for 20 epochs...\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20\n",
        "print(f\"\\nüèãÔ∏è  Training for {num_epochs} epochs...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xM8E-3jVDzx",
        "outputId": "7dd85683-474b-4d14-fa52-7c80b0366c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20, Loss: 0.4177\n",
            "\n",
            "üß™ Generated Text (Epoch 5):\n",
            "--------------------------------------------------\n",
            "The fundamental lawest annoons on hid prome,\n",
            "Who lets so fair a house fall to decay,\n",
            "Which husbandry in honeer,\n",
            "Who for that unf an the frape thou shouldst depart,\n",
            "Leaving thearey thy beauty‚Äôs legease,\n",
            "The sall that tho\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 10/20, Loss: 0.1587\n",
            "\n",
            "üß™ Generated Text (Epoch 10):\n",
            "--------------------------------------------------\n",
            "The fundamental law one hid sweets and steester-surd‚Äôst thou be distilled:\n",
            "Make sweet some vial; treasure thou some place,\n",
            "With beauty‚Äôs treasure ere it be self-killed:\n",
            "That use is not forbidden usury,\n",
            "Which happies tho\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 15/20, Loss: 0.1251\n",
            "\n",
            "üß™ Generated Text (Epoch 15):\n",
            "--------------------------------------------------\n",
            "As a scientist at MIT,\n",
            "And see the brave day sunk in hideous night,\n",
            "When I behold the violet past prime,\n",
            "And sable curls all silvered o‚Äôer with white:\n",
            "When lofty trees I see barren of leaves,\n",
            "Which erst from heat did canop\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 20/20, Loss: 0.1105\n",
            "\n",
            "üß™ Generated Text (Epoch 20):\n",
            "--------------------------------------------------\n",
            "The quantum field theory single life?\n",
            "Ah, if thou issueless shalt hap to die,\n",
            "The world will wail thee like a makeless wife,\n",
            "The world will be thy widow and still weep,\n",
            "That thou no form of thee hast left behind,\n",
            "When every \n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "  loss = trainer.train_epoch(dataloader)\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Generate sample text\n",
        "    start_prompts = [\n",
        "        \"The quantum field theory\",\n",
        "        \"As a scientist at MIT,\",\n",
        "        \"The fundamental law\",\n",
        "        \"Our research demonstrates\",\n",
        "        \"Mathematically, we can\"\n",
        "    ]\n",
        "\n",
        "    prompt = random.choice(start_prompts)\n",
        "    generated = trainer.generate_text(\n",
        "                start_text=prompt,\n",
        "                length=200,\n",
        "                temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüß™ Generated Text (Epoch {epoch+1}):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(generated)\n",
        "    print(\"-\" * 50)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV0ajEmMVGaW"
      },
      "outputs": [],
      "source": [
        "z`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNy8d2Q517FtQDETRqB++PU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}